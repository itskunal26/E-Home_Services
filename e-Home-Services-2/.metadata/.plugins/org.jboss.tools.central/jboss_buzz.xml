<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Debug memory errors with Valgrind and GDB</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/d30Kujr268w/debug-memory-errors-valgrind-and-gdb" /><author><name>Alexandra Petlanova Hajkova</name></author><id>bf30079c-6bb7-4320-9592-1e46df909769</id><updated>2021-11-01T07:00:00Z</updated><published>2021-11-01T07:00:00Z</published><summary type="html">&lt;p&gt;Buffer overflows, memory leaks, and similar memory issues plague many &lt;a href="https://developers.redhat.com/topics/c"&gt;C and C++&lt;/a&gt; programs. &lt;a href="https://www.valgrind.org/"&gt;Valgrind&lt;/a&gt; is a sophisticated utility for finding low-level programming errors, particularly involving memory use. The &lt;a href="https://www.gnu.org/software/gdb"&gt;GNU Project Debugger (GDB)&lt;/a&gt;, is a popular tool for use with C/C++ and other languages.&lt;/p&gt; &lt;p&gt;This article explains how to use Valgrind and GDB together along with &lt;a href="https://www.valgrind.org/docs/manual/manual-core-adv.html#manual-core-adv.vgdb"&gt;vgdb&lt;/a&gt;, a small program that serves as an intermediary between Valgrind and GDB. We'll debug a simple program containing various intentional bugs to demonstrate the combined usage of Valgrind, GDB, and vgdb. This buggy program writes to invalid memory, uses an uninitialized variable, and leaks memory. We will see how to combine Valgrind, GDB, and vgdb to find each of these bugs.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: It is also possible to use vgdb as an intermediary with the shell, although this usage will not be discussed.&lt;/p&gt; &lt;h2&gt;An example program with memory errors&lt;/h2&gt; &lt;p&gt;We will use the following error-filled program, called &lt;code&gt;bad_prog.c&lt;/code&gt;, as the basis for this article:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; typedef struct foo { int flag1; int flag2; int size; int **buf; } foo; void print_buf(struct foo *s) { printf("buf[0][0]: %d\n", s-&gt;buf[0][0]); free(s-&gt;buf); } void setup_foo(struct foo *s) { s-&gt;flag2 = 2; s-&gt;buf = malloc(20 * sizeof(int)); for (int i = 0; i &lt; 20; i++) s-&gt;buf[i] = malloc(20 * sizeof(int)); } int main(void) { struct foo s; setup_foo(&amp;s); print_buf(&amp;s); if (s.flag1 || s.flag2) printf("hey\n"); return 0; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Compile the program with the following &lt;code&gt;gcc&lt;/code&gt; command, which includes the necessary &lt;code&gt;-g&lt;/code&gt; option to enable debugging information:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gcc -g -o bad bad_prog.c&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When you run the program, it seems to behave nicely:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;./bad buf[20]: 0 hey&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when you run it under Valgrind, the code's problematic behaviors are uncovered. Valgrind produces output such as:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;valgrind -q ./bad ==1696253== Invalid write of size 8 ==1696253== at 0x4011E8: setup_foo (bad_prog.c:23) ==1696253== by 0x401210: main (bad_prog.c:30) ==1696253== Address 0x4e40090 is 0 bytes after a block of size 80 alloc'd ==1696253== at 0x4A36EA7: malloc (vg_replace_malloc.c:307) ==1696253== by 0x4011B4: setup_foo (bad_prog.c:21) ==1696253== by 0x401210: main (bad_prog.c:30) ==1696253== buf[0][0]: 82053872 ==1696253== Conditional jump or move depends on uninitialised value(s) ==1696253== at 0x401222: main (bad_prog.c:33) ==1696253== hey &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now let's combine Valgrind with GDB and take a closer look.&lt;/p&gt; &lt;h2&gt;Using Valgrind and GDB together&lt;/h2&gt; &lt;p&gt;Start up two terminal windows so that you can interact with Valgrind and GDB simultaneously.&lt;/p&gt; &lt;p&gt;In one terminal, run Valgrind with the &lt;code&gt;--vgdb-error=0&lt;/code&gt; option. When running with &lt;code&gt;--vgdb-error=&lt;/code&gt;&lt;em&gt;n&lt;/em&gt;, Valgrind waits for &lt;em&gt;n&lt;/em&gt; errors to occur before pausing and waiting for a connection from GDB. However, we want to be able to issue GDB commands before Valgrind finds any errors, so we use &lt;code&gt;--vgdb-error=0&lt;/code&gt;. The &lt;code&gt;--vgdb-error=0&lt;/code&gt; option causes Valgrind to pause before executing any instruction. Specifying &lt;code&gt;--vgdb-error&lt;/code&gt; with any value also enables &lt;code&gt;--vgdb=yes&lt;/code&gt;, which is required for using GDB with Valgrind.&lt;/p&gt; &lt;p&gt;Valgrind starts up with these messages:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ valgrind -q --vgdb-error=0 ./bad ==3781640== (action at startup) vgdb me ... ==3781640== ==3781640== TO DEBUG THIS PROCESS USING GDB: start GDB like this ==3781640== /path/to/gdb ./bad ==3781640== and then give GDB the following command ==3781640== target remote | /usr/local/lib/valgrind/../../bin/vgdb --pid=3781640 ==3781640== --pid is optional if only one valgrind process is running &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that Valgrind provides instructions for starting GDB as well as the command to use in order to connect GDB with Valgrind.&lt;/p&gt; &lt;p&gt;In the other terminal, run GBD as the Valgrind output instructed:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gdb ./bad ... Reading symbols from ./bad... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To connect to vgdb, follow the instructions written in the Valgrind output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) target remote | vgdb --pid=3781640 Remote debugging using | vgdb --pid=3781640 relaying data between gdb and process 3781640 warning: remote target does not support file transfer, attempting to access files from local filesystem. Reading symbols from /lib64/ld-linux-x86-64.so.2... Reading symbols from /usr/lib/debug/usr/lib64/ld-2.31.so.debug... 0x0000000004002110 in _start () from /lib64/ld-linux-x86-64.so.2 &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;--pid&lt;/code&gt; option is necessary only if you use vgdb with multiple Valgrind sessions.&lt;/p&gt; &lt;p&gt;Normally, when it starts, Valgrind places you in your program's &lt;code&gt;main&lt;/code&gt; function. Because the &lt;code&gt;--vgdb-error=0&lt;/code&gt; option causes Valgrind to pause before executing any instruction, we start in &lt;code&gt;ld.so&lt;/code&gt;, where the program is being set up to run.&lt;/p&gt; &lt;p&gt;Now you can use normal GDB commands, such as setting a breakpoint and running the program from the current point:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break main Breakpoint 1 at at 0x40120a: file bad_prog.c, line 30. (gdb) continue Continuing. Breakpoint 1, main () at bad_prog.c:30 30 setup_foo(&amp;s); (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. 0x00000000004011ed in setup_foo (s=0x1ffefff420) at bad_prog.c:23 23 s-&gt;buf[i] = malloc(20 * sizeof(int)); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;SIGTRAP&lt;/code&gt; signal informs us that something is wrong. Valgrind emulates a "trapping instruction" by generating a SIGTRAP signal. This SIGTRAP is sent by Valgrind only when attached to GDB, so that GDB can intercept the signal. Otherwise, Valgrind would simply print the error.&lt;/p&gt; &lt;h2&gt;Investigate an invalid write to memory&lt;/h2&gt; &lt;p&gt;Let's see whether there's something new at the terminal where vgdb is running:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;==875162== Invalid write of size 8 ==875162== at 0x4011ED: setup_foo (bad_prog.c:23) ==875162== by 0x401215: main (bad_prog.c:30) ==875162== Address 0x4e39090 is 0 bytes after a block of size 80 alloc'd ==875162== at 0x4A36EA7: malloc (vg_replace_malloc.c:307) ==875162== by 0x4011B9: setup_foo (bad_prog.c:21) ==875162== by 0x401215: main (bad_prog.c:30) ==875162== ==875162== (action on error) vgdb me ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Valgrind tells us we're writing beyond the end of a block of memory allocated with &lt;code&gt;malloc()&lt;/code&gt;. This example illustrates how can Valgrind and GDB work interactively through vgdb.&lt;/p&gt; &lt;p&gt;To find out what's wrong, we return to the GDB terminal. GDB already told us that the problem is happening during memory allocation:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;23 s-&gt;buf[i] = malloc(20 * sizeof(int));&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's print the value of &lt;code&gt;i&lt;/code&gt; and the addresses of our array members:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;print i $1 = 10 (gdb) p &amp;s-&gt;buf[i] $2 = (int **) 0x4e40090 (gdb) print sizeof (int) $3 = 4 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here we can see that the address of the &lt;code&gt;&amp;s-&gt;buf[10]&lt;/code&gt; array member is &lt;code&gt;0x4e40090&lt;/code&gt;. Valgrind told us that this address comes after the allocated block. Using this information, we see that the allocated memory block is too small.&lt;/p&gt; &lt;p&gt;In fact, this bug was caused by specifying the wrong type to the &lt;code&gt;sizeof&lt;/code&gt; operator. We requested &lt;code&gt;sizeof(int)&lt;/code&gt;, which on this particular architecture is 4 bytes, but the correct type is actually &lt;code&gt;int **&lt;/code&gt;, whose size is 8 bytes:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;print sizeof(int **) $4 = 8 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Discover uninitialized values&lt;/h2&gt; &lt;p&gt;Lets type &lt;code&gt;c&lt;/code&gt; to continue until we get back to our &lt;code&gt;main&lt;/code&gt; function:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Program received signal SIGTRAP, Trace/breakpoint trap. 0x0000000000401227 in main () at bad_prog.c:33 33 if (s.flag1 || s.flag2)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you look at the Valgrind terminal now, you see that Valgrind complains about uninitialized values:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;==566986== Conditional jump or move depends on uninitialised value(s) ==566986== at 0x401222: main (bad_prog.c:33) ==566986== ==566986== (action on error) vgdb me ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GDB has a &lt;a href="https://valgrind.org/docs/manual/manual-core-adv.html#manual-core-adv.valgrind-monitor-commands"&gt;monitor command&lt;/a&gt; that sends special requests to a remote target, in this case Valgrind. These requests include an extra set of commands that are specific to the target. Therefore, the commands shown in this article are specific to using GDB with Valgrind. The command won't be available under other circumstances.&lt;/p&gt; &lt;p&gt;To display the help for the &lt;code&gt;monitor&lt;/code&gt; command, type:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) monitor help general valgrind monitor commands: help [debug] : monitor command help. With debug: + debugging commands v.wait [ms] : sleep ms (default 0) then continue v.info all_errors : show all errors found so far ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;One useful &lt;code&gt;monitor&lt;/code&gt; command is &lt;code&gt;xb &lt;addr&gt; [&lt;len&gt;]&lt;/code&gt;, which prints information about the validity of &lt;code&gt;&lt;len&gt;&lt;/code&gt; bytes (the default is 1) at &lt;code&gt;&lt;addr&gt;&lt;/code&gt;. On the first line of output, the command prints a character that indicates whether the four bits at that position are defined. A &lt;em&gt;defined bit&lt;/em&gt; is one that the program has written, whereas an undefined bit hasn't been written to yet, or was overwritten with another undefined bit.&lt;/p&gt; &lt;p&gt;In the output from &lt;code&gt;monitor xb&lt;/code&gt;, a 0 indicates that the bits are defined, a 1 indicates that the bits are undefined, and a double underscore (__) indicates an unaddressable byte. On the second line of output, the command prints the addresses followed by the values at those addresses, in a layout similar to the GDB command &lt;code&gt;x/&lt;len&gt; &lt;addr&gt;&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The output from &lt;code&gt;xb&lt;/code&gt; is in hexadecimal, so &lt;code&gt;ff&lt;/code&gt; equals &lt;code&gt;1111 1111&lt;/code&gt;, indicating an undefined value. Let's use the &lt;code&gt;monitor xb&lt;/code&gt; command to check the &lt;code&gt;flag&lt;/code&gt; fields in &lt;code&gt;bad_prog&lt;/code&gt;. First, print the addresses of the flags and their sizes, then use this information in the &lt;code&gt;xb&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) print &amp;s.flag1 $4 = (int *) 0x1ffefff400 (gdb) print &amp;s.flag2 $5 = (int *) 0x1ffefff404 (gdb) print sizeof (s.flag1) $6 = 4 (gdb) monitor xb 0x1ffefff400 4 ff ff ff ff 0x1FFEFFF400: 0x50 0x12 0x40 0x00 (gdb) monitor xb 0x1ffefff404 4 00 00 00 00 0x1FFEFFF404: 0x02 0x00 0x00 0x00&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;flag1&lt;/code&gt; is all f's, which means that it is undefined. If we check the code, we see that only &lt;code&gt;flag2&lt;/code&gt; is assigned a value. The first line of output from &lt;code&gt;monitor xb&lt;/code&gt; for the address of &lt;code&gt;flag2&lt;/code&gt; consists of all zeros (&lt;code&gt;00 00 00 00&lt;/code&gt;), showing that the flag is defined.&lt;/p&gt; &lt;h2&gt;Scan for memory leaks&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;monitor leak_check&lt;/code&gt; command triggers a scan for memory leaks:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) monitor leak_check ==18002== 1,600 (+1,600) (1,440 (+1,440) direct, 160 (+160) indirect) bytes in 18 (+18) blocks are definitely lost in loss record 3 of 3 ==18002== at 0x4A36EA7: malloc (vg_replace_malloc.c:307) ==18002== by 0x4011EC: setup_foo (bad_prog.c:23) ==18002== by 0x401215: main (bad_prog.c:30) ==18002== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Valgrind tells us here that the function &lt;code&gt;setup_foo&lt;/code&gt;, which is called by &lt;code&gt;main&lt;/code&gt;, leaks memory allocated by &lt;code&gt;malloc()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) monitor block_list 3 ==18002== 1,600 (+1,600) (1,440 (+1,440) direct, 160 (+160) indirect) bytes in 18 (+18) blocks are definitely lost in loss record 3 of 3 ==18002== at 0x4A36EA7: malloc (vg_replace_malloc.c:307) ==18002== by 0x4011EC: setup_foo (bad_prog.c:23) ==18002== by 0x401215: main (bad_prog.c:30) ==18002== 0x4E390D0[80] ==18002== 0x4E39AF0[80] indirect loss record 1 ==18002== 0x4E39B80[80] indirect loss record 1 ... ==18002== 0x4E39160[80]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also ask for additional information about the pointers to memory that you think could be leaked. As an example, the variable &lt;code&gt;s&lt;/code&gt; is allocated in &lt;code&gt;main&lt;/code&gt;, so you would expect it to be on the stack. And indeed, the &lt;code&gt;montitor who_points_at&lt;/code&gt; command shows that the stack pointer &lt;code&gt;RSP&lt;/code&gt; points to that variable:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) print &amp;s $1 = (struct foo *) 0x1ffefff400 (gdb) monitor who_points_at 0x1ffefff400 ==18002== Searching for pointers to 0x1ffefff400 ==18002== tid 1 register RSP pointing at 0x1ffefff400 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, the information is not very interesting or useful, but when you're somewhere deep in the program and you suspect something has leaked, &lt;code&gt;who_points_at&lt;/code&gt; could be very helpful. It prints the list of references to a variable, which can help you find what variable you need to free.&lt;/p&gt; &lt;p&gt;It is possible to do the same with registers. Memcheck keeps a whole "shadow" memory copy of your program. This shadow copy keeps track of which bits are defined.&lt;/p&gt; &lt;p&gt;Valgrind also maintains &lt;em&gt;&lt;a href="https://valgrind.org/docs/manual/manual-core-adv.html#manual-core-adv.gdbserver-shadowregisters"&gt;shadow registers&lt;/a&gt;&lt;/em&gt;, which are "fake" registers that Valgrind provides to GDB to represent shadow bits. If you run Valgrind with the &lt;code&gt;--vgdb-shadow-registers=yes&lt;/code&gt; option, you can view Valgrind's shadow registers in GDB.&lt;/p&gt; &lt;p&gt;&lt;code&gt;monitor xb&lt;/code&gt; and &lt;code&gt; monitor get_vbits&lt;/code&gt; get information on undefined memory for any memory address in the program. The same can be done for everything held in a register.&lt;/p&gt; &lt;h2&gt;Command-line options&lt;/h2&gt; &lt;p&gt;You can set some Valgrind &lt;a href="https://valgrind.org/docs/manual/manual-core.html#manual-core.dynopts"&gt;command-line options&lt;/a&gt; through &lt;code&gt;monitor&lt;/code&gt; commands. For example, if you wish to start tracing system calls from this point on, enter:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) monitor v.clo --trace-syscalls=yes &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Ending the debugging session&lt;/h2&gt; &lt;p&gt;When you want to terminate the combined Valgrind and GDB debugging session, terminate the Valgrind process first via the GDB &lt;code&gt;kill&lt;/code&gt; command. After that, use &lt;code&gt;quit&lt;/code&gt; in GDB to exit the debugger:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) kill Kill the program being debugged? (y or n) y [Inferior 1 (Remote target) killed] (gdb) q&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;We've used Valgrind with GDB to find several programming errors related to memory use. First, we saw how Valgrind identified an invalid memory write and how GDB could be used to further diagnose this problem. Next, we looked for uninitialized data, and finally used GDB to ask Valgrind for a memory leak scan.&lt;/p&gt; &lt;p&gt;A pristine use of memory is crucial to providing secure, robust programs that don't break suddenly during use. The example in this article showed how a program could seem to perform properly in some situations while harboring serious memory flaws. Use Valgrind and GDB to protect your C and C++ code.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/01/debug-memory-errors-valgrind-and-gdb" title="Debug memory errors with Valgrind and GDB"&gt;Debug memory errors with Valgrind and GDB&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/d30Kujr268w" height="1" width="1" alt=""/&gt;</summary><dc:creator>Alexandra Petlanova Hajkova</dc:creator><dc:date>2021-11-01T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/11/01/debug-memory-errors-valgrind-and-gdb</feedburner:origLink></entry><entry><title>A beginner's attempt at optimizing GCC</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/G-ftfrM1m44/beginners-attempt-optimizing-gcc" /><author><name>Arjun Shankar</name></author><id>75f587c9-1273-469b-927c-a5a1474deba8</id><updated>2021-10-29T07:00:00Z</updated><published>2021-10-29T07:00:00Z</published><summary type="html">&lt;p&gt;Anyone who engages in &lt;a href="https://developers.redhat.com/topics/c"&gt;C/C++&lt;/a&gt; development on a modern &lt;a href="https://developers.redhat.com/topics/linux"&gt;GNU/Linux&lt;/a&gt; system typically ends up using the &lt;a href="https://gcc.gnu.org/"&gt;GCC&lt;/a&gt; or &lt;a href="https://llvm.org/"&gt;LLVM&lt;/a&gt; compiler, to which Red Hat actively contributes. As a member of the toolchain engineering team, I mostly work on runtime libraries (&lt;code&gt;glibc&lt;/code&gt;), but being acquainted with the internals of the compiler that builds &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) is useful.&lt;/p&gt; &lt;p&gt;With this goal in mind, and with a bit of advice from experienced engineers on the team, I decided to dip my toes into GCC internals by attempting to implement a small optimization. I was pointed to a list of open "tree optimization" bugs marked as &lt;code&gt;easyhack&lt;/code&gt;. These are beginner-friendly optimizations identified by the community.&lt;/p&gt; &lt;h2&gt;Finding a suitable GCC bug&lt;/h2&gt; &lt;p&gt;I went through a number of reports in the bug tracker. Some were already fixed by a recent commit and just needed closing, and others weren't nearly as simple as I'd like for my first attempt. Eventually, I found what seemed like an easy bug to work on, in the form of &lt;a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=96703"&gt;bug 96703&lt;/a&gt;, reported by Gabriel Ravier. The gist of the bug is that the following expression:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;(x &gt; y) &amp;&amp; (y == 0)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;can be simplified to:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;(x &gt; 0) &amp;&amp; (y == 0)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The optimization works because you're not interested in the result of &lt;code&gt;x &gt; y&lt;/code&gt; unless &lt;code&gt;y&lt;/code&gt; is zero.&lt;/p&gt; &lt;p&gt;In his initial comment Gabriel mentioned that, although this optimization doesn't appear to make the code run any faster, it &lt;em&gt;can&lt;/em&gt; potentially run faster on pipelined CPUs and make further optimizations easier. This is an optimization because it removes an "input dependence" between the two comparisons, and makes the first comparison depend only on one variable instead of two.&lt;/p&gt; &lt;p&gt;Clang/LLVM already implements this optimization, an advantage that happens to be a common thread in many of the bugs reported by Gabriel. It's nice to observe the healthy competition between the two compiler communities that leads to great compilers for everyone.&lt;/p&gt; &lt;h2&gt;Using GCC's Match and Simplify DSL&lt;/h2&gt; &lt;p&gt;So how would one go about implementing this optimization in GCC, as well? Thankfully, GCC comes with a purpose-built, domain-specific language (DSL) for writing expression simplifications like this one. It's described in the &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-7.2.0/gccint/Match-and-Simplify.html#Match-and-Simplify"&gt;Match and Simplify&lt;/a&gt; chapter of the GCC Internals manual.&lt;/p&gt; &lt;p&gt;The language is declarative. A new expression simplification only needs to be declared in &lt;code&gt;gcc/match.pd&lt;/code&gt;. The equivalent optimizer code is generated at GCC build time. A simplification takes the form of:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(simplify &lt;expression&gt; &lt;simplified_expression&gt;)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;An example from the GCC Internals manual is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(simplify (bit_and @0 integer_all_onesp) @0)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Items in this language are combined through &lt;em&gt;Polish&lt;/em&gt; or &lt;em&gt;prefix&lt;/em&gt; notation (along with a parenthesis-delimited syntax suspiciously reminiscent of Lisp). The &lt;code&gt;bit_and&lt;/code&gt; prefix represents the &lt;code&gt;&amp;&amp;&lt;/code&gt; that appears between expressions in C, but here &lt;code&gt;bit_and&lt;/code&gt; appears &lt;em&gt;before&lt;/em&gt; the two expressions instead of &lt;em&gt;between&lt;/em&gt; them.&lt;/p&gt; &lt;p&gt;This domain-specific code declares a simplification where the bitwise &lt;code&gt;AND&lt;/code&gt; of any operand (captured as &lt;code&gt;@0&lt;/code&gt;) and "all ones" (i.e., &lt;code&gt;0xFFFFFFFF&lt;/code&gt;) is equivalent to the operand &lt;code&gt;@0&lt;/code&gt; itself. &lt;code&gt;integer_all_onesp&lt;/code&gt; is a predicate that returns true for any operand that has all of its bits turned &lt;code&gt;ON&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;After looking around in &lt;code&gt;match.pd&lt;/code&gt; for inspiration, I could easily see that operations can be nested inside other operations, and that GCC will gladly accept, match, and simplify them. For the optimization I wanted to implement, the initial expression would be something like:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(bit_and (gt @0 @1) (eq @1 integer_zerop@2))&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;integer_zerop&lt;/code&gt; is another predicate, matching integers with the value 0. The &lt;code&gt;@2&lt;/code&gt; captures the operand so that it can be reused during the simplification if needed.&lt;/p&gt; &lt;p&gt;The optimized expression should look a bit like:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(bit_and (gt @0 ZERO) (eq @1 @2))&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this point, I didn't know how to emit 0 in the resulting expression. But once again, reading &lt;code&gt;match.pd&lt;/code&gt; gives the idea that &lt;code&gt;build_zero_cst (some_type)&lt;/code&gt; probably translates to a 0 value of the given type. Also, &lt;code&gt;TREE_TYPE (@0)&lt;/code&gt; returns the type of operand &lt;code&gt;@0&lt;/code&gt;. Combining these constructs leads to a simplified expression that GCC is likely to grok:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(bit_and (gt @0 { build_zero_cst (TREE_TYPE (@0)); }) (eq @1 @2); }))&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Putting all of this together, the final declaration of the simplification is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;/* (x &gt; y) &amp;&amp; (y == 0) -&gt; (x &gt; 0) &amp;&amp; (y == 0) */ (simplify (bit_and (gt @0 @1) (eq @1 integer_zerop@2)) (bit_and (gt @0 { build_zero_cst (TREE_TYPE (@0)); }) (eq @1 @2)))&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Adding this declaration to &lt;code&gt;match.pd&lt;/code&gt; and compiling it was all I needed to do to insert the necessary change into generated code. Now, let's see whether the change was truly an optimization.&lt;/p&gt; &lt;h2&gt;Did it work?&lt;/h2&gt; &lt;p&gt;Before I added the declaration, the following C code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;bool f(int x, int y) { return x &gt; y &amp;&amp; y == 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;translated (on x86-64) to:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;f: cmpl %esi, %edi # compare X and Y setg %al # set AL if X gt Y testl %esi, %esi # test Y sete %dl # set DL if Y eq 0 andl %edx, %eax # AL and DL ret&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After adding the declaration, GCC produces:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;f: testl %esi, %esi # test Y sete %al # set AL if Y equals 0 testl %edi, %edi # test X setg %dl # set DL if X gt 0 andl %edx, %eax # AL and DL ret&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The change breaks the input dependency, as I hoped, and brings the code in line with what Clang/LLVM generates.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Although my learning curve—from never having looked at &lt;code&gt;match.pd&lt;/code&gt; or tree optimizations to having hacked together my first optimization—was a bit steep, I found it was relatively easy for a beginner to implement a minor tree optimization using GCC's "Match and Simplify" infrastructure.&lt;/p&gt; &lt;p&gt;I have yet to properly test this new pattern, and there's possibly a fatal flaw in my approach. Also, there are several open questions, such as whether there is a more general pattern that optimizes for constants other than 0 and comparisons other than "greater than." I hope to answer these questions before I try submitting my very first patch. In the meantime, I'm happy to report that beginners can indeed quickly work toward their first contribution to GCC.&lt;/p&gt; &lt;p&gt;Happy hacking!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/29/beginners-attempt-optimizing-gcc" title="A beginner's attempt at optimizing GCC"&gt;A beginner's attempt at optimizing GCC&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/G-ftfrM1m44" height="1" width="1" alt=""/&gt;</summary><dc:creator>Arjun Shankar</dc:creator><dc:date>2021-10-29T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/29/beginners-attempt-optimizing-gcc</feedburner:origLink></entry><entry><title type="html">WildFly S2I v2 architecture overview</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/kD0HMhd92v8/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2021/10/29/wildfly-s2i-v2-overview/</id><updated>2021-10-29T00:00:00Z</updated><content type="html">For upcoming 26 release we are re-architecting images to offer more flexibility, better efficiency and a simpler user experience. Since WildFly 16, in which we introduced an evolution of the WildFly s2i images, we have identified a set of pain points that we want to see addressed in a new architecture: * Strong coupling of the WildFly S2I builder image with a WildFly release. This implies that new builder images have to be released and deployed for each new server version. * Configuring the server during the S2I build is complex. Executing the simplest WildFly CLI script during build is far from trivial; it requires your application project to comply with a specific directory structure then you must define bash scripts (do you know Java developers who like to write bash scripts?) in order to call CLI command lines (and don’t forget to start the embedded server). The simplest copy of some extra server content (e.g. JBoss Module modules) is also not that trivial and again requires bash scripting. * Executing your own CLI scripts during server startup implies having installed some extensions (again bash scripts) inside the server during the S2I build. * Server startup is always composed of a sequence of 2 steps — server starts first in admin only mode and automatically applies generated CLI scripts, then reboots in normal mode. In cases your server configuration doesn’t require some adjustments and could have booted directly in normal mode. * No simple way to provide server arguments (e.g. Java system properties) when starting the server. * No way to tailor a server according to the application needs from the project pom.xml file. Server provisioning during the S2I build is configured by env variables provided at S2I build time. * No way to build and run an application with a server. * WildFly S2I images contain much more than what we actually need to build and run the server. We need a much lighter Operating System. NEW WILDFLY S2I ARCHITECTURE The current architecture is composed of 2 main artifacts: * WildFly S2I builder image with JDK11 based on centos7. Image released for each new WildFly release (to contain the latest release of the WildFly server). * WildFly S2I runtime image with JDK11 based on centos7. Image used to create docker chained builds to output smaller runtime application images. We are keeping the separation between the S2I builder image and the runtime image. The 2 new images we are offering are: * WildFly S2I builder image with JDK11 based on . This image doesn’t contain a WildFly server. It expects a server containing the application deployment to be provisioned during the S2I build phase. * WildFly runtime image with JDK11 based on . We are keeping the JDK (instead of a JRE) in order to make Java debugging tools available in the image. This image can be used to run any WildFly server (not only a server provisioned using the builder image in a chained build). For now we will only be providing JDK 11 versions of these images. So the WildFly S2I builder image becomes a lightweight generic image allowing to build and execute applications deployed in any WildFly server. NEW S2I BUILD WORKFLOW We are removing the complex server configuration points and rely on the use of the that can now provision a fully configured server containing your deployment. The WildFly Maven plugin 3.0.0.Alpha1 has been evolved with some new goals to provision, configure, and package the server and the deployment in one step. When designing your application pom file, add the WildFly Maven plugin package goal, configure it with the and , and optionally reference WildFly CLI scripts to be executed and content to be copied inside the server. At the end of the build you will get (by default in the target/server directory) a server with your app deployed, ready to be installed in the image. In order to allow for a smooth transition to the new images, we are still supporting (in a deprecated way) the legacy workflow. Your existing application would work, but you are now required to specify the Galleon feature-pack(s) and layer(s) (GALLEON_PROVISION_FEATURE_PACKS and GALLEON_PROVISION_LAYERS env variables) you want to see used during the S2I build to provision a WildFly server. NEW IMAGE RUNTIME API An image built from the WildFly S2I builder or runtime images both expose the same API allowing you to fine tune the server execution. This API is exposed by means of environment variables to be set when configuring your deployment. JVM CONFIGURATION API The JVM that are used today with WildFly s2i images are still supported. They are a nice way to tune the JVM. WILDFLY SERVER STARTUP CONFIGURATION API The new server startup configuration API is described in this . This API comes with default values that should cover the main use-cases. 2 env variables open-up new possibilities: * SERVER_ARGS allows you to pass WildFly server arguments when starting the server. * CLI_LAUNCH_SCRIPT allows you to provide a path (relative to JBOSS_HOME or absolute) to a CLI script to be executed at startup time. Although CLI scripts should be executed at build time from the WildFly Maven plugin, in some cases it can be useful to adjust the configuration at execution time. You can package a set of CLI scripts inside your server at build time, then reference one of these CLI scripts to be executed at runtime. WILDFLY SERVER SUBSYSTEMS CONFIGURATION API If you are using WildFly s2i images you are perhaps asking yourself where are the env variables you have been using to configure the elytron subsystem, to add datasources, to configure logging or the microprofile-config subsystem,… They are provided by means of a new that you can combine with the WildFly Galleon feature-pack at build time to produce a server supporting these env variables. * If you only provision org.wildfly:wildfly-galleon-pack:25.0.0.Final you will get a "vanilla" WildFly server that will get lightly adjusted by the image entry-point to properly execute on OpenShift. * If you provision org.wildfly:wildfly-galleon-pack:25.0.0.Final and org.wildfly.cloud:wildfly-cloud-galleon-pack:1.0.0.Alpha2 you will get a similar server to the one present in the current WildFly s2i image (with JBOSS_HOME/bin/openshift-launch.sh launcher). EXAMPLES You can pull the new WildFly S2i images (Beta quality) from quay.io: * docker pull quay.io/jfdenise/wildfly-s2i-jdk11 * docker pull quay.io/jfdenise/wildfly-runtime-jdk11 NB: The images will be made available from the quay.io/wildfly organisation when they reach a Final quality. S2I EXAMPLES We have defined a set of to help you get started. They cover different use-cases that highlight the new capabilities. The examples rely on to automate the build and deployment on OpenShift. In order to deploy the examples onto OpenShift, you can log in to the . The use cases covered are: * , no specific configuration. Just build and deploy on OpenShift. * . Use WildFly 25 elytron-oidc-client to interact with a Keycloak server. Also highlights the ability to provide server arguments at launch time. * . We all need to enable logging at some point. With a simple CLI script executed at server boot time, enable logging and redirect all traces to the CONSOLE. * . A cluster of PODS that share web sessions. This example benefits from the WildFly cloud feature-pack and WildFly Helm Charts capabilities to automatically enable the dns.DNS_PING JGroups protocol and generate the ping service. DOCKERFILE EXAMPLE This chapter highlights the steps to build a docker image that contains the server and your application, publish it in a public docker registry in which you have an account (e.g. ) and then deploy it on OpenShift. Here we are using the example. NB: Be sure to update the example steps with your own docker registry account. * Build the maven project: $ mvn clean package * Write a Dockerfile with the following content: FROM quay.io/jfdenise/wildfly-runtime-jdk11:latest COPY --chown=jboss:root target/server $JBOSS_HOME RUN chmod -R ug+rwX $JBOSS_HOME * Build the image $ docker build -t quay.io/jfdenise/my-app:latest . * You can run the image locally and interact with the application (e.g. ). $ docker run --rm quay.io/jfdenise/my-app:latest * Publish the image $ docker push quay.io/jfdenise/my-app:latest * Write a WildFly Heml Charts my-app.yaml file with the following content: image: name: quay.io/jfdenise/my-app build: enabled: false * Deploy on OpenShift helm install my-app -f my-app.yaml wildfly_v2/wildfly * Once deployed, access the application route URL (e.g. ) TO CONCLUDE We hope that, like us, you will see the benefits of this new approach (for which you can perhaps find similarities with the S2I experience). This is going to allow us to offer more flexibility (provision the server of your choice), better efficiency (smaller images, faster server startup), simpler user experience (WildFly Maven plugin configuration is far simpler than the existing S2I configuration points). So in the end a much better overall experience. Keep us posted with your feedback. (You can log these as new .) This will help us evolve the new WildFly S2I experience in the right direction. Thank-you! JF Denise&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/kD0HMhd92v8" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-François Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/10/29/wildfly-s2i-v2-overview/</feedburner:origLink></entry><entry><title>Consume Pino logs from Node.js applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/SIj6wRDhXNc/consume-pino-logs-nodejs-applications" /><author><name>Ash Cripps</name></author><id>1be17e56-397c-4e80-9b83-885fcc6c0ecb</id><updated>2021-10-28T07:00:00Z</updated><published>2021-10-28T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; offers a vast array of options to developers. This is why Red Hat and IBM teamed up to produce the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture&lt;/a&gt;, a series of recommendations to help you build Node.js applications in the cloud. One of our recommendations is that you use &lt;a href="https://getpino.io/"&gt;Pino&lt;/a&gt;, an object logger for Node.js. You can visit &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/logging.md"&gt;this GitHub page&lt;/a&gt; for an overview of how and why to use Pino. This article demonstrates how to create and consume Pino logs with the &lt;a href="https://docs.openshift.com/container-platform/4.7/logging/cluster-logging.html"&gt;Red Hat OpenShift Logging service&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow along, you need a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster and a Node.js application you can deploy to OpenShift. For our example, we'll use the &lt;a href="https://github.com/nodeshift-starters/nodejs-circuit-breaker"&gt;nodejs-circuit-breaker&lt;/a&gt; from &lt;a href="https://github.com/nodeshift"&gt;NodeShift&lt;/a&gt;, a collection of tools maintained by Red Hat for Node.js developers.&lt;/p&gt; &lt;h2&gt;Installing OpenShift Logging&lt;/h2&gt; &lt;p&gt;To deploy OpenShift Logging, we'll install two operators: The OpenShift Elasticsearch Operator and the OpenShift Logging Operator.&lt;/p&gt; &lt;p&gt;To install the OpenShift Elasticsearch Operator:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;In the OpenShift web console, open &lt;strong&gt;OperatorHub &lt;/strong&gt;under the Operators submenu.&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;OpenShift Elasticsearch Operator&lt;/strong&gt; and click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Double-check that the &lt;strong&gt;All namespaces on the cluster&lt;/strong&gt; option is selected.&lt;/li&gt; &lt;li&gt;For an installed namespace, select &lt;strong&gt;openshift-operators-redhat&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select the option to enable recommended monitoring on this namespace.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Wait for the operator to install.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;This operator installs both the &lt;a href="https://www.elastic.co/elasticsearch/"&gt;Elasticsearch&lt;/a&gt; text data store and its &lt;a href="https://www.elastic.co/guide/en/kibana/master/kuery-query.html"&gt;Kibana&lt;/a&gt; visualization tool, which serve as the backbone of the OpenShift Logging system.&lt;/p&gt; &lt;p&gt;After the Elasticsearch Operator is installed, install the OpenShift Logging Operator as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Navigate back to the &lt;strong&gt;OperatorHub&lt;/strong&gt; and select the &lt;strong&gt;OpenShift Logging Operator&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select a specific namespace, then &lt;strong&gt;openshift-logging&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select the option to enable recommended monitoring on this namespace.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Wait for the operator to install.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The key component installed with this operator is the OpenShift Log Forwarder, which sends logs to the Elasticsearch instance. The Log Forwarder takes the container logs from every pod in every namespace and forwards them to the namespace and containers running Elasticsearch. This communication allows the logs to flow where you can analyze them without requiring each container to have a certificate and route set up to access the separate namespace containing Elasticsearch.&lt;/p&gt; &lt;h2&gt;Deploying OpenShift Logging&lt;/h2&gt; &lt;p&gt;Now that you have the building blocks installed via operators, you will deploy the pods containing the logging system. To do this you need a &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"&gt;custom resource definition (CRD)&lt;/a&gt;, a configuration concept in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This CRD defines what and how many pods you need, where to install them, and key setup features for the Elasticsearch instance, such as the size of the disk and the retention policy. The following YAML code is an example CRD for deploying the logging infrastructure:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: "logging.openshift.io/v1" kind: "ClusterLogging" metadata: name: "instance" namespace: "openshift-logging" spec: managementState: "Managed" logStore: type: "elasticsearch" retentionPolicy: application: maxAge: 1d infra: maxAge: 7d audit: maxAge: 7d elasticsearch: nodeCount: 3 storage: storageClassName: size: 200G resources: requests: memory: "8Gi" proxy: resources: limits: memory: 256Mi requests: memory: 256Mi redundancyPolicy: "SingleRedundancy" visualization: type: "kibana" kibana: replicas: 1 curation: type: "curator" curator: schedule: "30 3 * * *" collection: logs: type: "fluentd" fluentd: {}&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: OpenShift Logging is not designed to be a long-term storage solution. This example stores its logs for only seven days before deletion. For long-lived logs, you need to change the &lt;code&gt;retentionPolicy&lt;/code&gt; property and the storage type under &lt;code&gt;storageClassName&lt;/code&gt;. For more information on how to set up suitable storage for long-lived logs, please &lt;a href="https://docs.openshift.com/container-platform/4.8/logging/config/cluster-logging-log-store.html#cluster-logging-elasticsearch-ha_cluster-logging-store"&gt;refer to the documentation.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To create the CRD:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Navigate to &lt;strong&gt;Custom Resource Definitions&lt;/strong&gt; under the &lt;strong&gt;Administration&lt;/strong&gt; tab in the sidebar. Search for "ClusterLogging" and click on the result.&lt;/li&gt; &lt;li&gt;On this page, click on &lt;strong&gt;Actions&lt;/strong&gt; and then &lt;strong&gt;View Instances&lt;/strong&gt; (the page might need a refresh to load). Then click &lt;strong&gt;Create.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Replace the YAML code there with the YAML from the preceding example and click &lt;strong&gt;Create&lt;/strong&gt; again.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;To check the installation's progress, navigate to the pods page. The page should show three Elasticsearch pods spinning up, along with the Kibana pod and some &lt;a href="https://www.fluentd.org/"&gt;Fluentd&lt;/a&gt; pods that support logging. These pods will take a few minutes to spin up.&lt;/p&gt; &lt;h2&gt;Enabling JSON parsing&lt;/h2&gt; &lt;p&gt;As explained at the beginning of this article, we use Pino for logging in our sample Node.js application. To most effectively use the log data generated by Pino, you need to ensure that the OpenShift Logging Operator can parse the JSON data correctly. JSON parsing is possible as of version 5.1 of this operator. You only need to deploy a custom &lt;code&gt;ClusterLogForwarder&lt;/code&gt; resource. This will overwrite the Fluentd pods and provide the configuration needed to parse JSON logs. The configuration is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: logging.openshift.io/v1 kind: ClusterLogForwarder metadata: name: instance namespace: openshift-logging spec: outputDefaults: elasticsearch: structuredTypeKey: kubernetes.pod_name pipelines: - inputRefs: - application - infrastructure - audit name: all-to-default outputRefs: - default parse: json&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;structuredTypeKey&lt;/code&gt; property determines how the new indexes are split up. In this example, the forwarder creates a new index for each pod that has its logs forwarded to Elasticsearch.&lt;/p&gt; &lt;h2&gt;Generating the Node.js logs&lt;/h2&gt; &lt;p&gt;Next, you'll push the application to generate logs from the NodeShift starter repository.&lt;/p&gt; &lt;p&gt;In a terminal, clone the repository and change into the directory installed:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone git@github.com:nodeshift-starters/nodejs-circuit-breaker.git $ cd nodejs-circuit-breaker&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before deploying your application, &lt;a href="https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html#cli-logging-in_cli-developer-commands"&gt;log in to your OpenShift cluster&lt;/a&gt;. Logging in requires a token, which you can retrieve from the OpenShift user interface (UI) by clicking on &lt;strong&gt;Copy login command&lt;/strong&gt; from the user drop-down menu in the top right corner. This gives you a command similar to:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc login --token=$TOKEN --server=$SERVER:6443&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After logging in, run the deployment script to deploy the application to OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./start-openshift.sh&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Deployment takes a few minutes. You can check progress from the Topology overview in the &lt;strong&gt;Developer&lt;/strong&gt; console. Once the services are deployed, you can start viewing your logs.&lt;/p&gt; &lt;h2&gt;Viewing the Node.js logs&lt;/h2&gt; &lt;p&gt;To view your logs, first set up a Kibana instance as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Inside the OpenShift UI, click the nine squares at the top right and then select logging.&lt;/li&gt; &lt;li&gt;Accept the permissions required by the service account.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;This takes you to your Kibana page, where you have to do a few things before viewing data.&lt;/p&gt; &lt;p&gt;The first task is to set up an index pattern so you can view the data. Enter "&lt;code&gt;app-nodejs*&lt;/code&gt;" for the pattern. Thanks to the trailing asterisk, the pattern allows you to view all logs from any application that uses "&lt;code&gt;nodejs&lt;/code&gt;" in its naming convention for its pods. The prepended string "&lt;code&gt;app&lt;/code&gt;" is from the &lt;code&gt;ClusterLogForwarder&lt;/code&gt;, to indicate that this index came from an application pod.&lt;/p&gt; &lt;p&gt;Select &lt;strong&gt;Timestamp&lt;/strong&gt; as the time filter field.&lt;/p&gt; &lt;p&gt;That's all you need to retrieve the logs.&lt;/p&gt; &lt;p&gt;Now, select &lt;strong&gt;Discover&lt;/strong&gt; at the top left, which displays all the logs inside your Elasticsearch instance. Here, you can filter through all the logs and look for specific logs from certain pods.&lt;/p&gt; &lt;p&gt;Because the index pattern I've suggested here matches logs from indexes belonging to my "nodejs" apps, I only have three logs, as shown in Figure 1. If I go down the left-hand side and select all the "structured." fields, the display shows only the parsed JSON in my Kibana results. These are the fields you can search on, making the most of your JSON logging.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/logs.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/logs.png?itok=4zXdkVKK" width="1440" height="721" alt="An example of the Kibana output showing only logs from three Node.js applications." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Kibana output, showing the logs selected by filtering for Node.js applications. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article was an introduction to using OpenShift's built-in cluster logging to consume Pino logs from your Node.js applications. We installed both the Elasticsearch Operator and the OpenShift Logging Operator, then deployed the OpenShift default Elasticsearch service and a custom &lt;code&gt;ClusterLogForwarder&lt;/code&gt;, all of which enabled us to collate all of our application logs.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/28/consume-pino-logs-nodejs-applications" title="Consume Pino logs from Node.js applications"&gt;Consume Pino logs from Node.js applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/SIj6wRDhXNc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ash Cripps</dc:creator><dc:date>2021-10-28T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/28/consume-pino-logs-nodejs-applications</feedburner:origLink></entry><entry><title type="html">Codeanywhere adventures - Getting started with developer decision management tooling (part 4)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3NkFpgRhj5Y/codeanywhere-adventures-getting-started-with-developer-decision-management-tooling.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/Y_Ro4di3aWY/codeanywhere-adventures-getting-started-with-developer-decision-management-tooling.html</id><updated>2021-10-28T05:00:00Z</updated><content type="html"> In the , we introduced the world of , a cloud IDE and container development experience all available in just your browser.  Are you ready for some more amazing, easy to use, developer tooling that requires not a single tooling installation and no configuration?  That's what the team at  are promising us when I stumbled on their website last week. They "...don't require you to engage in complex installations and configuration setups. Simply access our in-browser IDE for everything you need to build amazing websites in a productive and more developer-friendly way." In a final part four of this series, we'll setup a complete decision management Java container project in . Instead of covering how to create a Java container project to get started in this article, please follow the steps outlined in  and this time name the container project DECISION MANAGEMENT  TOOLING. When you complete the creation of your container project, you should be logged in and in your . From there you can find your DECISION MANAGEMENT TOOLING container listing and use the OPEN IDE button to create a tab with the  IDE and your project. Make sure you give it enough time to create and start up before you proceed. Now we have an empty container project, so let's load it up with our decision manager project instead of the process automation demonstrated in part two. Again, as a Gitlab user, I'm going to use that console at the bottom of my IDE tab and clone my project into the workspace: $ git clone https://gitlab.com/bpmworkshop/rhdm-install-demo.git You should now see the project is already in the IDE (in a new tab of your browser) and automatically recognised by the EXPLORER view of our workspace. If we view the READM in this project we'll see that there are setup steps that install it on a local machine or you have the option to install it in a container using Podman on your local machine. As we are using the cloud IDE for this, we are going to consider our container project the local machine and use those instructions. Having already cloned the project to our IDE, we then see that it's going to require a few Red Hat products that we can pull from the Red Hat Developers site. The products we need to install process automation tooling are listed in the installs/README file and include: * JBoss EAP 7.3.0 (jboss-eap-7.3.0.zip) * Red Hat Decision Central 7.11 deployable (rhdm-7.11.0-decision-central-eap7-deployable.zip) * Red Hat Decision Manager KIE server 7.11 (rhdm-7.11.0-kie-server-ee8.zip) We can pull these into the project by finding the , needing only to have a registered user to access them. Locate the correct JBoss EAP 7.3.0 and two Red Hat Decision Manager downloads, and pull them to your local machine by clicking on their download buttons.  After that we need to push them with secure copy to the project installs directory in our container project. This is done by finding out what user I am on this container in the cloud IDE console: $ whoami cabox Now assuming you are in the same directory as the downloaded product files on your local machine, from a console run the following to copy the files to your container project installs directory: $ scp -P 31828 jboss-eap-7.3.0.zip cabox@host31.codeanyhost.com:~/workspace/rhdm-install-demo/installs/ cabox@host31.codeanyhost.com: Permission denied (publickey). lost connection This means we do not have access until we share our public key with the hosting container. Generating SSH keys is beyond the scope of this article, but assuming you have one, copy and past it into the file via your cloud IDE console in the file ~/.ssh/authorized_keys.  Once that's done you can now again try to copy the file securely and should see successful results so copy over all three files you downloaded above: $ scp -P 31828 *.zip cabox@host31.codeanyhost.com:~/workspace/rhdm-install-demo/installs/ At this point, you're ready to install the decision management developer tooling, so in your  IDE console make sure you're in the root directory of the rhdm-install-demo project and run the following: $ cd $HOME/workspace/rhdm-install-demo; ./init.sh You should see the installation script run and end with the login details... but we have to remember we are using the  IDE container project and refer back to the GETING STARTED page that was opened originally that included special URL's to access our applications. Before we start the server, note that there is a line in the GETTING STARTED page in our IDE project that stated "To access your web application make sure your application server is running and listening on 0.0.0.0 address..." This indicates that the normal setup for a JBoss EAP server needs to be adjusted as it's default configuration is to listen on 127.0.0.1 or localhost.  Open the file rhdm-install-demo/target/jboss-eap-7.3/standalone/configuration/standalone.xml and change all instances of 127.0.0.1 to 0.0.0.0, then close and save the file. Now we're ready to start the decision management tooling with the following command: $ ./target/jboss-eap-7.3/bin/standalone.sh We have to wait on the complete startup of the server, and then instead of localhost:8080 we need to replace the presented URL with the GETTING STARTED suggestions. This means we should find our decision management tooling on the following handy links that are provided as the server starts listening to ports (note you can find these under the PREVIEW PORTS link at the bottom right of the IDE): While the OPEN BROWSER links for port 8080 does not exist, use the 8001 link and edit the port number in the URL. This give us the JBoss EAP server admin console, we need to add a forward slash with the decision central application added to reach the decision management tooling log in: https://port-8080-decision-management-tooling--eric863427.preview.codeanywhere.com/decision-central Follow the rest of the project README file to find out how to log in to the tooling and for links to workshops that help you get started with developing your first real decision management project. This completes the final part four of our  adventures, where we installed, deployed, and accessed developer decision management tooling as a cloud IDE container experience.  As this completes the series, be sure to go back and enjoy any of the previous articles if you missed them. Happy cloud developing with  adventures!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3NkFpgRhj5Y" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/Y_Ro4di3aWY/codeanywhere-adventures-getting-started-with-developer-decision-management-tooling.html</feedburner:origLink></entry><entry><title type="html">Keycloak.X Update</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cisB8OHj4Eg/keycloak-x-update" /><author><name>Stian Thorgersen</name></author><id>https://www.keycloak.org/2021/10/keycloak-x-update</id><updated>2021-10-28T00:00:00Z</updated><content type="html">It’s been quite some time since we announced the plans around Keycloak.X, two years in fact. Due to other priorities we’ve been a bit distracted, but now it’s finally full speed ahead. Keycloak.X will be lighter, faster, easier, more scalable, more cloud native, and a bunch of other things. Expect greatness coming your way! As part of Keycloak.X we’re not only making code changes, but there will also be a cultural shift where the team behind Keycloak will focus a lot more on user experience and the delivery of a manageable solution over simply pieces of code. There will be some disruptive changes coming, but we will strive to make the transition as easy as possible for everyone. For breaking changes such as moving from WildFly to Quarkus we plan to provide 6 months to do the migration. If that is not enough there is , which is a supported build of Keycloak by Red Hat. 7, which is based on current Keycloak architecture, has support until (currently says 2023, but will soon be extended to 2024). We will follow-up to this blog post with more details in the future, but for now let’s look at some of the highlights coming to Keycloak.X. HIGHLIGHTS EXPERIENCES As mentioned previously a lot more attention will be put on your experience with Keycloak. With this in mind we have identified a few experiences that we believe cover a wide range of different use-cases: * App developer Developers that are integrating Keycloak with applications and services * Customizer Developers that are extending Keycloak or integrating with other systems * Bridge Using Keycloak as a bridge between applications and other identity solutions * Regular A typical small to medium-sized deployment of Keycloak * Super-sized Elastic and highly available deployment of Keycloak for very large use-cases * SaaS A extension to super-sized where Keycloak enables identities for SaaS, CIAM, and B2C scenarios QUARKUS We’re switching to Quarkus as the platform to build Keycloak. Compared to WildFly this gives faster startup-time and lower memory footprint. It also provides a much simpler approach to configuring Keycloak, with command-line arguments and environment variables instead of complicated XML files. Another great aspect of Quarkus is that it gives us a lot more control over what external libraries are included in the distribution, including faster upgrades of dependencies, which should significantly improve on situation around CVEs. STORAGE RE-ARCHITECTURE We’re doing a significant re-architecture of the storage layer as part of Keycloak.X to address a number of shortcomings that where discovered in the current architecture. Zero downtime upgrade, scalability, and availability will be key topics of this new architecture, as well as making it a lot easier to support additional storage types in the long run. OPERATOR AND CONTAINERS With the current approach to configuration in Keycloak creating a good experience around a container is problematic as the container has to convert from environment variables to complicated XML configuration files. With the work we’re doing around Quarkus configuring Keycloak with environment variables becomes a native thing, making it a lot simpler to provide a great container experience. Similarly, the Operator can also be made simpler as it will be easier to configure Keycloak, as well as having better opinionated configuration from the base distribution, which trickles through from the Zip distribution, to the container, and finally to the Operator. To align the codebase more we’re also re-writing the Operator from scratch using the Java SDK and Quarkus. OBSERVABILITY Metrics, tracing, logging, and health-checks are all important aspect of a cloud native application. These are all important capabilities to manage and debug Keycloak in production, especially when running on Kubernetes or OpenShift. GITOPS FRIENDLY CONFIGURATION In a GitOps or CI/CD environment it can be problematic to manage the runtime configuration within Keycloak. As all configuration such as realms and clients live in the database and can only be managed through REST APIs it is hard to reliably manage as part of a GitOps process. Along with the storage re-architecture comes a very powerful capability that can federate configuration from multiple sources, and we plan to take advantage of this with a file-based store, where Keycloak can read more static/immutable configuration from the file-system (YAML of course), and combine this with dynamic/mutable configuration from the DB. Further, this enables checking in your static configuration in a Git repository, and deploy it to your development, stage and production environments as needed. EXTERNAL INTEGRATIONS Keycloak has a large number of extension points today, called SPIs. With Java (and in some cases JavaScript) it is possible to customize Keycloak with custom providers for these SPIs. Although, highly powerful and flexible, this is not ideal in a modern Kuberetes centric architecture. As the extensions are co-located with Keycloak it is harder to deploy, upgrade, and scale extensions. Extensions can also not be written in any language or framework making it more costly for non-Java developers to extend Keycloak. With this in mind we are planning more focus on the ability to extend and integrate with Keycloak through remote extensions, and are looking at REST, gRPC, Knative, Kafka, etc. as vehicles to achieve this. In addition we would also like to get to a point where we can have a "headless" Keycloak allowing a frontend to be built in any way you want, which would bring a great addition to the current themes approach to customising the UI. DECOMPOSING Last, but not least. We are also planning on ability to decompose Keycloak as well as bring better isolation on Keycloak’s code base and capabilities. We’re not planning to go full micro-service architecture here, but rather a sensible compromise allowing everything to run as a single process, with the ability to separate some parts of Keycloak into external services. ROADMAP As you can imagine all of what we have planned in Keycloak.X is a large amount of work, and won’t happen overnight. We’re focusing first on the breaking changes such as moving to Quarkus and re-architecture of the storage layer. Everything is not planned fully at this point, but we do have some idea of when we believe the various components of Keycloak.X will be delivered. * ASAP: Keycloak 16 will be the last preview of the Quarkus distribution, so we welcome everyone to try it out, and provide us with . * December 2021: In Keycloak 17 we will make the Quarkus distribution fully supported, and deprecate the WildFly distribution. * March 2022: In Keycloak 18 we are aiming to include the new Operator, and preview the new store. We’re also planning on removing WildFly support from the code-base at this point. * June 2022: First release with only the Quarkus distribution. We’re also hoping to make the new store a fully supported option at this point. The dates above are subject to change! FEEDBACK We would love your feedback on our plans around Keycloak.X, so please join us on to discuss the future of Keycloak!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cisB8OHj4Eg" height="1" width="1" alt=""/&gt;</content><dc:creator>Stian Thorgersen</dc:creator><feedburner:origLink>https://www.keycloak.org/2021/10/keycloak-x-update</feedburner:origLink></entry><entry><title type="html">Infinispan Operator 2.2.0.Final</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cJ2ua3v7aQc/infinispan-operator-2-2-final" /><author><name>Ryan Emerson</name></author><id>https://infinispan.org/blog/2021/10/27/infinispan-operator-2-2-final</id><updated>2021-10-27T12:00:00Z</updated><content type="html">We’re pleased to announce for Kubernetes and Red Hat OpenShift. This is the first Operator release based on Infinispan 13. Release highlights: * Custom server configuration. Add custom configuration for Infinispan Server using ConfigMap objects. * Configurable number of relay nodes for cross-site replication. Relay nodes send and receive replication requests from backup locations. You can now increase the number of relay nodes with the sites.local.maxRelayNodes field to achieve a better distribution of cross-site replication requests. * TLS security for cross-site replication traffic. You can now encrypt cross-site connections between Infinispan clusters with TLS by adding keystore secrets and configuring the sites.local.encryption field. * Operator SDK upgraded to v1.3.2&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cJ2ua3v7aQc" height="1" width="1" alt=""/&gt;</content><dc:creator>Ryan Emerson</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/10/27/infinispan-operator-2-2-final</feedburner:origLink></entry><entry><title>A compiler option, a hidden visibility, and a weak symbol walk into a bar</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CWaxrPz8bYg/compiler-option-hidden-visibility-and-weak-symbol-walk-bar" /><author><name>Serge Guelton</name></author><id>d1cfaebe-31ed-4e63-add8-743f9a855e39</id><updated>2021-10-27T07:00:00Z</updated><published>2021-10-27T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://llvm.org/"&gt;LLVM&lt;/a&gt; packaging team recently ran into a &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;compiler&lt;/a&gt; problem. A build of the LLVM package with &lt;a href="https://clang.llvm.org/"&gt;Clang&lt;/a&gt;, with link-time optimization activated, failed validation. This article steps through how we explored, identified, and ultimately fixed the problem.&lt;/p&gt; &lt;h2&gt;The LLVM package build&lt;/h2&gt; &lt;p&gt;The build on the &lt;a href="https://fedoraproject.org/wiki/Releases/Rawhide"&gt;Fedora Rawhide version&lt;/a&gt; should be an easy task at the packaging level:&lt;/p&gt; &lt;pre&gt;&lt;code class="diff"&gt;diff --git a/llvm.spec b/llvm.spec (...) @@ -1,3 +1,5 @@ +%global toolchain clang (...) +BuildRequires: clang &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let's introduce the characters in the mystery to follow.&lt;/p&gt; &lt;h2&gt;Disabling runtime type information&lt;/h2&gt; &lt;p&gt;By default, LLVM compiles with the &lt;code&gt;-fno-rtti&lt;/code&gt; option, which disables runtime type information. According to the LLVM coding standard, the compiler disables the information to &lt;a href="https://llvm.org/docs/CodingStandards.html#do-not-use-rtti-or-exceptions"&gt;reduce code and executable size&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Yet, sometimes, a type must be associated with a unique identifier. One example involves &lt;code&gt;llvm::Any&lt;/code&gt;, the LLVM version of &lt;code&gt;std::any&lt;/code&gt;. A typical implementation of &lt;code&gt;std::any&lt;/code&gt; involves &lt;code&gt;typeid&lt;/code&gt;, as showcased by &lt;a href="https://github.com/llvm/llvm-project/blob/6adbc83ee9e46b476e0f75d5671c3a21f675a936/libcxx/include/any#L293"&gt;the &lt;code&gt;libcxx&lt;/code&gt; version&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;typeid&lt;/code&gt; operator cannot be used with the &lt;code&gt;-fno-rtti&lt;/code&gt; option, so we had to find an alternative. The current implementation of &lt;code&gt;llvm::Any&lt;/code&gt; and &lt;code&gt;std::any&lt;/code&gt; can be mocked by the following snippet, mapping &lt;code&gt;&amp;TypeId&lt;MyType&gt;::Id&lt;/code&gt; to a unique identifier:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;template &lt;typename T&gt; struct TypeId { static const char Id; }; &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Hidden visibility&lt;/h2&gt; &lt;p&gt;Parts of LLVM are compiled with &lt;code&gt;-fvisibility=hidden&lt;/code&gt;. This option forces the default visibility of all symbols to be &lt;code&gt;hidden&lt;/code&gt;, which prevents them from being visible across library boundaries. Hiding symbols offers better control over exported symbols in a shared library.&lt;/p&gt; &lt;p&gt;What happens when the &lt;code&gt;TypeId&lt;/code&gt; construct from the previous section is combined with hidden visibility? Let's compile two shared libraries out of the same code:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;template &lt;typename T&gt; struct TypeId { static const char Id; }; template &lt;typename T&gt; const char TypeId&lt;T&gt;::Id = 0; #ifdef FOO const char* foo() { return &amp;TypeId&lt;int&gt;::Id; } #else const char* bar() { return &amp;TypeId&lt;int&gt;::Id; } #endif &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We compile two binaries, one with &lt;code&gt;FOO&lt;/code&gt; defined and one without, to carry out the different &lt;code&gt;#ifdef&lt;/code&gt; paths:&lt;/p&gt; &lt;pre&gt;&lt;code class="sh"&gt;&gt; clang++ -DFOO foo.cpp -shared -o libfoo.so &gt; clang++ foo.cpp -shared -o libbar.so &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Without hidden visibility, both libraries place our &lt;code&gt;Id&lt;/code&gt; at the same address:&lt;/p&gt; &lt;pre&gt;&lt;code class="sh"&gt;&gt; llvm-nm -C libfoo.so (...) 0000000000000679 V TypeId&lt;int&gt;::Id &gt; llvm-nm -C libbar.so (...) 0000000000000679 V TypeId&lt;int&gt;::Id &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;V&lt;/code&gt; in the output indicates that the symbol is a weak object, which means that only one of the items will be chosen by the linker. Therefore, we keep unicity of the symbol and its address across compilation units.&lt;/p&gt; &lt;p&gt;But when compiled with &lt;code&gt;-fvisibility-hidden&lt;/code&gt;, the symbols no longer are weak:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;&gt; clang++ -fvisibility=hidden foo.cpp -shared -o libbar.so &gt; llvm-nm -C libfoo.so (...) 0000000000000629 r TypeId&lt;int&gt;::Id &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;r&lt;/code&gt; next to the address means that the symbol is in a read-only data section. The symbol is not dynamically linked (as we can confirm from the output of &lt;code&gt;llvm-nm -D&lt;/code&gt;), so it gets different addresses in &lt;code&gt;libfoo&lt;/code&gt; and &lt;code&gt;libbar&lt;/code&gt;. In short, unicity is not preserved.&lt;/p&gt; &lt;h2&gt;Fine-grained control over symbol visibility&lt;/h2&gt; &lt;p&gt;A straightforward fix for the incompatibility we've uncovered is to explicitly state that &lt;code&gt;TypeId::Id&lt;/code&gt; must always have the default visibility. We can make this change as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;template &lt;typename T&gt; struct __attribute__((visibility("default"))) TypeId { static const char Id; }; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's check that the fix works:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;&gt; clang++ -fvisibility=hidden foo.cpp -shared -o libbar.so &gt; llvm-nm -C libfoo.so (...) 0000000000000659 V TypeId&lt;int&gt;::Id &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;V&lt;/code&gt; for a weak symbol has returned, but that's not the end of the story.&lt;/p&gt; &lt;p&gt;Instead of parameterizing &lt;code&gt;TypeId&lt;/code&gt; by &lt;code&gt;int&lt;/code&gt;, let's parameterize it by a &lt;code&gt;HiddenType&lt;/code&gt; class declared with hidden visibility:&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;struct HiddenType {}; template &lt;typename T&gt; struct __attribute__((visibility("default"))) TypeId { static const char Id; }; template &lt;typename T&gt; const char TypeId&lt;T&gt;::Id = 0; const char* foo() { return &amp;TypeId&lt;HiddenType&gt;::Id; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When compiling this code with &lt;code&gt;-fvisibility-hidden&lt;/code&gt;, where does &lt;code&gt;TypeId&lt;HiddenType&gt;::Id&lt;/code&gt; end up?&lt;/p&gt; &lt;pre&gt;&lt;code class="cplusplus"&gt;&gt; clang++ -fvisibility=hidden foo.cpp -shared -o libbar.so &gt; llvm-nm -CD libbar.so | grep -c TypeId&lt;HiddenType&gt;::Id 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Fascinating! This exercise shows that a template function with &lt;em&gt;default&lt;/em&gt; visibility, instantiated with a type of &lt;em&gt;hidden&lt;/em&gt; visibility, ends up with &lt;em&gt;hidden&lt;/em&gt; visibility. Indeed, flagging &lt;code&gt;HiddenType&lt;/code&gt; with &lt;code&gt;__attribute__((visibility("default")))&lt;/code&gt; restores the expected behavior.&lt;/p&gt; &lt;h2&gt;Where theory meets LLVM&lt;/h2&gt; &lt;p&gt;Once we isolated the behavior described in the preceding section, we could easily provide the relevant patches in LLVM:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://reviews.llvm.org/D101972"&gt;Force visibility of &lt;code&gt;llvm::Any&lt;/code&gt; to external&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://reviews.llvm.org/D108943"&gt;Fine grain control over some symbol visibility&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://reviews.llvm.org/D109252"&gt;Add extra check for &lt;code&gt;llvm::Any::TypeId&lt;/code&gt; visibility&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;These patches fix the build issue mentioned at the beginning of the article and ensure that it won't reproduce, which is the kind of outcome programmers always look for.&lt;/p&gt; &lt;h2&gt;Acknowledgments&lt;/h2&gt; &lt;p&gt;The author would like to thank Béatrice Creusillet, Adrien Guinet, and the editorial team for their help on this article.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/27/compiler-option-hidden-visibility-and-weak-symbol-walk-bar" title="A compiler option, a hidden visibility, and a weak symbol walk into a bar"&gt;A compiler option, a hidden visibility, and a weak symbol walk into a bar&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CWaxrPz8bYg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Serge Guelton</dc:creator><dc:date>2021-10-27T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/27/compiler-option-hidden-visibility-and-weak-symbol-walk-bar</feedburner:origLink></entry><entry><title>Quarkus 2.4.0.Final released - Hibernate Reactive 1.0.0, Kafka Streams DevUI, Multi module continuous testing, AWT image resize via new AWT extension and much more</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/MNE00QjR5gQ/" /><author><name>Alexey Loubyansky (https://twitter.com/aloubyansky)</name></author><id>https://quarkus.io/blog/quarkus-2-4-0-final-released/</id><updated>2021-10-27T00:00:00Z</updated><published>2021-10-27T00:00:00Z</published><summary type="html">Today, we release Quarkus 2.4.0.Final which includes a lot of refinements and improvements and some new features: Hibernate Reactive 1.0.0.Final Introducing Kafka Streams DevUI Support continuous testing for multi module projects Support AWT image resize via new AWT extension Migration Guide To migrate from 2.3, please refer to our migration...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/MNE00QjR5gQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Alexey Loubyansky (https://twitter.com/aloubyansky)</dc:creator><dc:date>2021-10-27T00:00:00Z</dc:date><feedburner:origLink>
                https://quarkus.io/blog/quarkus-2-4-0-final-released/
            </feedburner:origLink></entry><entry><title type="html">Counterfactuals; getting the right answer</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aspIGNLHIXg/counterfactuals-getting-the-right-answer.html" /><author><name>Roberto Emanuel</name></author><id>https://blog.kie.org/2021/10/counterfactuals-getting-the-right-answer.html</id><updated>2021-10-26T13:54:07Z</updated><content type="html">Sometimes the result of an automated decision may be neither desired or that which was required. What if there was a tool to find a way to overturn those decisions, maybe changing some of the figures that were provided to the system, and achieve a different outcome? That’s what we’ve been working on lately within the Trusty AI initiative. We added a new experimental feature to the Audit Investigation console called Counterfactual Analysis. Counterfactuals allow for the outcome of a decision to be set and a range of viable inputs to be searched over until the outcome is achieved. Let’s see how it works in detail with a practical example. GETTING AN APPROVAL FOR A DENIED LOAN We will consider a model responsible to approve or deny mortgage applications. The model produces two outputs, specifically Mortgage Approval (obviously) and a Risk Score associated to the loan request. We have submitted a request to the model, so we go to the Audit investigation dashboard and we open the execution detail to discover that the mortgage request was denied. Let’s open the Counterfactual tool using the new tab available in the navigation menu. At this point we want to specify a different outcome for the decision. To do so, we click on the "Set Up Outcomes" button. A modal window will open where we can see the original outcomes and we can change them according to our desire. We’ll set "Mortgage Approval" to True and we’ll check "Automatically adjust for counterfactual" for the Risk score. In this way the analysis will search for any risk score value that produces a Mortgage approval. Then we’ll click on "Confirm". Now we have to specify how we would like to alter the inputs in order to achieve the outcomes we’ve just chosen. In the CF table we see the list of all the inputs provided within the execution and their original value. Clicking on the checkbox at the beginning of an input row will set the input as changeable. Then clicking on its "Constraint" button we will have to set a range of values for the selected input. The system will search for values within the provided range to find solutions matching the desired outcome. So, we will enable the "TotalRequired" input and we’ll set up a range constraint from 0 to 100,000. We are basically searching what’s the amount that could be loaned given the provided salary and assets. At this point we’ve filled out all the required information to run the analysis. We’ve selected at least one outcome different from the original one (Mortgage Approval, from false to true) and we provided at least one searchable input (the requested amount). RUNNING THE ANALYSIS We are all set and we can click on "Run analysis" to start looking for solutions. The default analysis will run for one minute. At the end of it we can see that the system was able to find some results! The best solution found is showed at the far left of the results area and it’s marked with a star near its ID. We can see that the mortgage request could be approved for a "TotalRequired" sum around 18,500. At this point we could try another run by allowing other inputs to change. By clicking on "Edit Counterfactual" it’s possible to start over keeping the search options already provided. We could eventually try searching with higher values for "TotalAsset" or "MonthlySalary" for example. That’s it! Our brief introduction to the Counterfactual Analysis ends here. Keep in mind that it is still an experimental feature at the moment. It only supports a limited set of types for the outcomes and the inputs of a decision (only numbers and booleans). The Counterfactual Analysis tool will be available with the upcoming release of Kogito 1.13. You can learn more about running TrustyAI in the . FURTHER READINGS If you are interested in reading more about how counterfactuals work behind the hood you can explore the following resources: * * * The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aspIGNLHIXg" height="1" width="1" alt=""/&gt;</content><dc:creator>Roberto Emanuel</dc:creator><feedburner:origLink>https://blog.kie.org/2021/10/counterfactuals-getting-the-right-answer.html</feedburner:origLink></entry></feed>
